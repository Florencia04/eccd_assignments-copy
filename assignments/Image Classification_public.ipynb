{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50feb14a",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "The objective of this notebook is to offer a first approximation at Image Classification problems.\n",
    "\n",
    "For this, we will be using a very popular library `PyTorch` and a DataSet of fruits and vegetables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb571383",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nv https://raw.githubusercontent.com/danski3456/notebook_grading/main/utils.py -O utils.py\n",
    "%run utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea2338",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eccd_datasets > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the original lables used when training a resnet\n",
    " \n",
    "import json\n",
    "!wget https://files.fast.ai/models/imagenet_class_index.json -O resnet_labels.json\n",
    "with open(\"resnet_labels.json\", \"r\") as fh: data = json.load(fh)\n",
    "resnet_labels = {}\n",
    "for k, v in data.items():\n",
    "    resnet_labels[int(k)] = v[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDENT_NAME = \"Nombre Apellido\"\n",
    "COURSE_NAME = \"eccd-feb22\"\n",
    "EXERCISE_NAME = \"image-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e95e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import models\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from eccd_datasets import load_images\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f77c9",
   "metadata": {},
   "source": [
    "# Exploring the dataset\n",
    "\n",
    "First, we invite you to go to the dataset folder and explore the content and structure of the project.\n",
    "\n",
    "The dataset used in this notebook consists on a subset from the dataset located [here](https://github.com/marcusklasson/GroceryStoreDataset)\n",
    "\n",
    "Once that is done, we can start looking at what is included in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2237c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = load_images()\n",
    "df_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0cb455",
   "metadata": {},
   "source": [
    "### Looking at the images\n",
    "\n",
    "We can use the `PIL` library to look at the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b554f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(image_data):  \n",
    "    return Image.open(io.BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c700ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image_data(df_images.iloc[0][\"image_data\"])\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb71927e",
   "metadata": {},
   "source": [
    "### Images as matrices\n",
    "\n",
    "We can also look look at the matrix representation of each image using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2302a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.array(image)\n",
    "print(\"Image shape\", I.shape)\n",
    "print(f\"Image range in each coordinate: [{I.min()}, {I.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0153d581",
   "metadata": {},
   "source": [
    "And we can modify the image manually by changing the values of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a62ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_I = I.copy()\n",
    "new_I[:, :, 0] = 0 # Killing the red channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(new_I.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be1f65",
   "metadata": {},
   "source": [
    "# PyTorch Transformations\n",
    "\n",
    "The same way we normalize tabular data with Standard and MinMax scalers, we need to normalize image data.\n",
    "\n",
    "We will proceed to explore some of the most used transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05dfb6",
   "metadata": {},
   "source": [
    "## Resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transf.Resize((100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdc0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = transformation.forward(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2992492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resized_image)\n",
    "plt.title(f\"New shape: {np.array(resized_image).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b94c0f",
   "metadata": {},
   "source": [
    "## Center Crop\n",
    "\n",
    "Implement a transformation for croping and centering (hint: there is a transformation that does that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0319f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop_transformation(image, size: int) -> np.array:\n",
    "    \"\"\"\n",
    "    This function uses a pytorch transformation to\n",
    "    center and crop the image\n",
    "    \"\"\"\n",
    "    # Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7731e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_center_crop = center_crop_transformation(image, 150)\n",
    "plt.imshow(answer_center_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert answer_center_crop.shape == (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee1745",
   "metadata": {},
   "source": [
    "## RandomResizedCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc70800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image original size: \", np.array(image).shape)\n",
    "fig, ax = plt.subplots(1, 6, figsize=(20, 4))\n",
    "for i, size in enumerate([50, 100, 150, 200, 300, 500]):\n",
    "  \n",
    "    transformation = transf.RandomResizedCrop(size)\n",
    "\n",
    "    crp_img = transformation.forward(image)\n",
    "    ax[i].imshow(np.array(crp_img))\n",
    "    ax[i].set_title(np.array(crp_img).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312dbb74",
   "metadata": {},
   "source": [
    "## Random Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transf.RandomHorizontalFlip()\n",
    "\n",
    "maybe_flipped = [transformation.forward(image) for _ in range(5)]\n",
    "\n",
    "plt.imshow(np.hstack([np.array(img) for img in maybe_flipped]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ef41db",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "The same way we normalize columns for tabular data, here we normalize each image according to the mean and standard deviation of each colour channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e5cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_images = [load_image_data(row[\"image_data\"]) for _, row in df_images.iloc[:2].iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_image_dataset = np.array([np.array(img) for img in two_images])\n",
    "two_image_dataset.shape\n",
    "\n",
    "plt.imshow(np.hstack([np.array(img) for img in two_images]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(two_image_dataset, axis=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143321c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transf.Compose([\n",
    "        transf.ToTensor(),\n",
    "        transf.Normalize(\n",
    "            np.mean(two_image_dataset, axis=(0, 1, 2)),\n",
    "            np.std(two_image_dataset, axis=(0, 1, 2)),\n",
    "            )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_two_image_dataset = [transformation(img) for img in two_image_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c5a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_two_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f46da",
   "metadata": {},
   "source": [
    "# Using ImageNet\n",
    "\n",
    "Since training a large neural network requires lots of data and computing power, often we download a pre-trained neural network, which we can later fine-tune.\n",
    "\n",
    "Here, we will download an ImageNet network.\n",
    "\n",
    "Remember that since the network is already trained with a specific dataset, when evaluating new images, we need transform them using the same transformations used for training. In particular, that includes using the same normalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa372f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acec593",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d154a02",
   "metadata": {},
   "source": [
    "We load a maping from resnet integer labels to the actual categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_resnet(image):\n",
    "    \"\"\"\n",
    "    This image uses the resnet as is to\n",
    "    predict an image.\n",
    "    Remember to apply the correct transformations\n",
    "    to the image before feeding it to the network.\n",
    "    \n",
    "    The following link might be useful: https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e56b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = (\n",
    "    load_image_data(\n",
    "        df_images[\n",
    "            df_images[\"coarse_cat\"] == \"Apple\"\n",
    "        ]\n",
    "        .iloc[0]\n",
    "        [\"image_data\"]\n",
    "    )\n",
    "\n",
    ")\n",
    "img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fe0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = (\n",
    "    load_image_data(\n",
    "        df_images[\n",
    "            df_images[\"coarse_cat\"] == \"Orange\"\n",
    "        ]\n",
    "        .iloc[0]\n",
    "        [\"image_data\"]\n",
    "    )\n",
    "\n",
    ")\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dd85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = (\n",
    "    load_image_data(\n",
    "        df_images[\n",
    "            df_images[\"coarse_cat\"] == \"Pear\"\n",
    "        ]\n",
    "        .iloc[0]\n",
    "        [\"image_data\"]\n",
    "    )\n",
    "\n",
    ")\n",
    "img3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = predict_using_resnet(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pred1 == \"lemon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e147f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = predict_using_resnet(img2)\n",
    "print(pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = predict_using_resnet(img3)\n",
    "print(pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "proposed_solution = {\n",
    "'attempt': {\n",
    "    'course_name': COURSE_NAME,\n",
    "    'exercise_name': EXERCISE_NAME,\n",
    "    'username': STUDENT_NAME,\n",
    "},\n",
    "'task_attempts': \n",
    "[\n",
    "         {\n",
    "            \"name\": \"center_crop\",\n",
    "            \"answer\": np.array_str(answer_center_crop),\n",
    "         },\n",
    "\n",
    "         {\n",
    "            \"name\": \"resnet_pred2\",\n",
    "            \"answer\": pred2,\n",
    "         },\n",
    "         {\n",
    "            \"name\": \"resnet_pred3\",\n",
    "            \"answer\": pred3,\n",
    "         }\n",
    "]\n",
    "\n",
    "}\n",
    "check_solution(proposed_solution)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
